{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning Recipes with Josh Gordon"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[machine-learning-recipes with Josh Gordon](https://www.youtube.com/playlist?list=PLOU2XLYxmsIIuiBfYad6rFYQU_jL2ryal)\n",
    "[Github-Josh Gordon](https://github.com/random-forests)\n",
    "[中文字幕video-machine-learning-recipes with Josh Gordon](http://www.mooc.ai/course/96/learn?lessonid=634#lesson/634)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[machine-learning-recipes学习资源和代码](http://blog.csdn.net/fyuanfena/article/details/52312961)\n",
    "[Github-machine-learning-recipes学习资源和代码](https://github.com/fyuanfen/Machine-Learning-Recipe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# -*- coding: UTF-8 -*-\n",
    "from __future__ import division\n",
    "#from __future__ import print_function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-04T01:08:55.672959Z",
     "start_time": "2018-01-04T01:08:55.316390Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1]\n"
     ]
    }
   ],
   "source": [
    "# Part 1 - Hello World - https://youtu.be/cKxRvEZd3Mw\n",
    "\n",
    "# Follow a recipe for supervised learning (a technique to create a classifier from examples) and code it up.\n",
    "\n",
    "from sklearn import tree\n",
    "\n",
    "# Examples\n",
    "# Weight Texture Label\n",
    "# 150g   Bumpy   Orange\n",
    "# 170g   Bumpy   Orange\n",
    "# 140g   Smooth  Apple\n",
    "# 130g   Smooth  Apple\n",
    "\n",
    "# Training Data\n",
    "# features = [[140, \"smooth\"], [130, \"smooth\"], [150, \"bumpy\"], [170, \"bumpy\"]]  # Input to classifier\n",
    "features = [[140, 1], [130, 1], [150, 0], [170, 0]]  # scikit-learn uses real-valued features\n",
    "# labels = [\"apple\", \"apple\", \"orange\", \"orange\"]  # Desired output\n",
    "labels = [0, 0, 1, 1]\n",
    "\n",
    "# Train Classifer\n",
    "clf = tree.DecisionTreeClassifier()  # Decision Tree classifier\n",
    "clf = clf.fit(features, labels)  # Find patterns in data\n",
    "\n",
    "# Make Predictions\n",
    "print clf.predict([[160, 0]])\n",
    "# Output: 0-apple, 1-orange\n",
    "# Correct output is: 1-orange"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-04T01:14:42.564742Z",
     "start_time": "2018-01-04T01:14:42.087302Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['DESCR', 'data', 'feature_names', 'target', 'target_names']\n",
      "<class 'sklearn.utils.Bunch'>\n",
      "['sepal length (cm)', 'sepal width (cm)', 'petal length (cm)', 'petal width (cm)']\n",
      "['setosa' 'versicolor' 'virginica']\n",
      "[ 5.1  3.5  1.4  0.2]\n",
      "0\n",
      "Example 0: label 0, features [ 5.1  3.5  1.4  0.2]\n",
      "Example 1: label 0, features [ 4.9  3.   1.4  0.2]\n",
      "Example 2: label 0, features [ 4.7  3.2  1.3  0.2]\n",
      "Example 3: label 0, features [ 4.6  3.1  1.5  0.2]\n",
      "Example 4: label 0, features [ 5.   3.6  1.4  0.2]\n",
      "Example 5: label 0, features [ 5.4  3.9  1.7  0.4]\n",
      "Example 6: label 0, features [ 4.6  3.4  1.4  0.3]\n",
      "Example 7: label 0, features [ 5.   3.4  1.5  0.2]\n",
      "Example 8: label 0, features [ 4.4  2.9  1.4  0.2]\n",
      "Example 9: label 0, features [ 4.9  3.1  1.5  0.1]\n",
      "Example 10: label 0, features [ 5.4  3.7  1.5  0.2]\n",
      "Example 11: label 0, features [ 4.8  3.4  1.6  0.2]\n",
      "Example 12: label 0, features [ 4.8  3.   1.4  0.1]\n",
      "Example 13: label 0, features [ 4.3  3.   1.1  0.1]\n",
      "Example 14: label 0, features [ 5.8  4.   1.2  0.2]\n",
      "Example 15: label 0, features [ 5.7  4.4  1.5  0.4]\n",
      "Example 16: label 0, features [ 5.4  3.9  1.3  0.4]\n",
      "Example 17: label 0, features [ 5.1  3.5  1.4  0.3]\n",
      "Example 18: label 0, features [ 5.7  3.8  1.7  0.3]\n",
      "Example 19: label 0, features [ 5.1  3.8  1.5  0.3]\n",
      "Example 20: label 0, features [ 5.4  3.4  1.7  0.2]\n",
      "Example 21: label 0, features [ 5.1  3.7  1.5  0.4]\n",
      "Example 22: label 0, features [ 4.6  3.6  1.   0.2]\n",
      "Example 23: label 0, features [ 5.1  3.3  1.7  0.5]\n",
      "Example 24: label 0, features [ 4.8  3.4  1.9  0.2]\n",
      "Example 25: label 0, features [ 5.   3.   1.6  0.2]\n",
      "Example 26: label 0, features [ 5.   3.4  1.6  0.4]\n",
      "Example 27: label 0, features [ 5.2  3.5  1.5  0.2]\n",
      "Example 28: label 0, features [ 5.2  3.4  1.4  0.2]\n",
      "Example 29: label 0, features [ 4.7  3.2  1.6  0.2]\n",
      "Example 30: label 0, features [ 4.8  3.1  1.6  0.2]\n",
      "Example 31: label 0, features [ 5.4  3.4  1.5  0.4]\n",
      "Example 32: label 0, features [ 5.2  4.1  1.5  0.1]\n",
      "Example 33: label 0, features [ 5.5  4.2  1.4  0.2]\n",
      "Example 34: label 0, features [ 4.9  3.1  1.5  0.1]\n",
      "Example 35: label 0, features [ 5.   3.2  1.2  0.2]\n",
      "Example 36: label 0, features [ 5.5  3.5  1.3  0.2]\n",
      "Example 37: label 0, features [ 4.9  3.1  1.5  0.1]\n",
      "Example 38: label 0, features [ 4.4  3.   1.3  0.2]\n",
      "Example 39: label 0, features [ 5.1  3.4  1.5  0.2]\n",
      "Example 40: label 0, features [ 5.   3.5  1.3  0.3]\n",
      "Example 41: label 0, features [ 4.5  2.3  1.3  0.3]\n",
      "Example 42: label 0, features [ 4.4  3.2  1.3  0.2]\n",
      "Example 43: label 0, features [ 5.   3.5  1.6  0.6]\n",
      "Example 44: label 0, features [ 5.1  3.8  1.9  0.4]\n",
      "Example 45: label 0, features [ 4.8  3.   1.4  0.3]\n",
      "Example 46: label 0, features [ 5.1  3.8  1.6  0.2]\n",
      "Example 47: label 0, features [ 4.6  3.2  1.4  0.2]\n",
      "Example 48: label 0, features [ 5.3  3.7  1.5  0.2]\n",
      "Example 49: label 0, features [ 5.   3.3  1.4  0.2]\n",
      "Example 50: label 1, features [ 7.   3.2  4.7  1.4]\n",
      "Example 51: label 1, features [ 6.4  3.2  4.5  1.5]\n",
      "Example 52: label 1, features [ 6.9  3.1  4.9  1.5]\n",
      "Example 53: label 1, features [ 5.5  2.3  4.   1.3]\n",
      "Example 54: label 1, features [ 6.5  2.8  4.6  1.5]\n",
      "Example 55: label 1, features [ 5.7  2.8  4.5  1.3]\n",
      "Example 56: label 1, features [ 6.3  3.3  4.7  1.6]\n",
      "Example 57: label 1, features [ 4.9  2.4  3.3  1. ]\n",
      "Example 58: label 1, features [ 6.6  2.9  4.6  1.3]\n",
      "Example 59: label 1, features [ 5.2  2.7  3.9  1.4]\n",
      "Example 60: label 1, features [ 5.   2.   3.5  1. ]\n",
      "Example 61: label 1, features [ 5.9  3.   4.2  1.5]\n",
      "Example 62: label 1, features [ 6.   2.2  4.   1. ]\n",
      "Example 63: label 1, features [ 6.1  2.9  4.7  1.4]\n",
      "Example 64: label 1, features [ 5.6  2.9  3.6  1.3]\n",
      "Example 65: label 1, features [ 6.7  3.1  4.4  1.4]\n",
      "Example 66: label 1, features [ 5.6  3.   4.5  1.5]\n",
      "Example 67: label 1, features [ 5.8  2.7  4.1  1. ]\n",
      "Example 68: label 1, features [ 6.2  2.2  4.5  1.5]\n",
      "Example 69: label 1, features [ 5.6  2.5  3.9  1.1]\n",
      "Example 70: label 1, features [ 5.9  3.2  4.8  1.8]\n",
      "Example 71: label 1, features [ 6.1  2.8  4.   1.3]\n",
      "Example 72: label 1, features [ 6.3  2.5  4.9  1.5]\n",
      "Example 73: label 1, features [ 6.1  2.8  4.7  1.2]\n",
      "Example 74: label 1, features [ 6.4  2.9  4.3  1.3]\n",
      "Example 75: label 1, features [ 6.6  3.   4.4  1.4]\n",
      "Example 76: label 1, features [ 6.8  2.8  4.8  1.4]\n",
      "Example 77: label 1, features [ 6.7  3.   5.   1.7]\n",
      "Example 78: label 1, features [ 6.   2.9  4.5  1.5]\n",
      "Example 79: label 1, features [ 5.7  2.6  3.5  1. ]\n",
      "Example 80: label 1, features [ 5.5  2.4  3.8  1.1]\n",
      "Example 81: label 1, features [ 5.5  2.4  3.7  1. ]\n",
      "Example 82: label 1, features [ 5.8  2.7  3.9  1.2]\n",
      "Example 83: label 1, features [ 6.   2.7  5.1  1.6]\n",
      "Example 84: label 1, features [ 5.4  3.   4.5  1.5]\n",
      "Example 85: label 1, features [ 6.   3.4  4.5  1.6]\n",
      "Example 86: label 1, features [ 6.7  3.1  4.7  1.5]\n",
      "Example 87: label 1, features [ 6.3  2.3  4.4  1.3]\n",
      "Example 88: label 1, features [ 5.6  3.   4.1  1.3]\n",
      "Example 89: label 1, features [ 5.5  2.5  4.   1.3]\n",
      "Example 90: label 1, features [ 5.5  2.6  4.4  1.2]\n",
      "Example 91: label 1, features [ 6.1  3.   4.6  1.4]\n",
      "Example 92: label 1, features [ 5.8  2.6  4.   1.2]\n",
      "Example 93: label 1, features [ 5.   2.3  3.3  1. ]\n",
      "Example 94: label 1, features [ 5.6  2.7  4.2  1.3]\n",
      "Example 95: label 1, features [ 5.7  3.   4.2  1.2]\n",
      "Example 96: label 1, features [ 5.7  2.9  4.2  1.3]\n",
      "Example 97: label 1, features [ 6.2  2.9  4.3  1.3]\n",
      "Example 98: label 1, features [ 5.1  2.5  3.   1.1]\n",
      "Example 99: label 1, features [ 5.7  2.8  4.1  1.3]\n",
      "Example 100: label 2, features [ 6.3  3.3  6.   2.5]\n",
      "Example 101: label 2, features [ 5.8  2.7  5.1  1.9]\n",
      "Example 102: label 2, features [ 7.1  3.   5.9  2.1]\n",
      "Example 103: label 2, features [ 6.3  2.9  5.6  1.8]\n",
      "Example 104: label 2, features [ 6.5  3.   5.8  2.2]\n",
      "Example 105: label 2, features [ 7.6  3.   6.6  2.1]\n",
      "Example 106: label 2, features [ 4.9  2.5  4.5  1.7]\n",
      "Example 107: label 2, features [ 7.3  2.9  6.3  1.8]\n",
      "Example 108: label 2, features [ 6.7  2.5  5.8  1.8]\n",
      "Example 109: label 2, features [ 7.2  3.6  6.1  2.5]\n",
      "Example 110: label 2, features [ 6.5  3.2  5.1  2. ]\n",
      "Example 111: label 2, features [ 6.4  2.7  5.3  1.9]\n",
      "Example 112: label 2, features [ 6.8  3.   5.5  2.1]\n",
      "Example 113: label 2, features [ 5.7  2.5  5.   2. ]\n",
      "Example 114: label 2, features [ 5.8  2.8  5.1  2.4]\n",
      "Example 115: label 2, features [ 6.4  3.2  5.3  2.3]\n",
      "Example 116: label 2, features [ 6.5  3.   5.5  1.8]\n",
      "Example 117: label 2, features [ 7.7  3.8  6.7  2.2]\n",
      "Example 118: label 2, features [ 7.7  2.6  6.9  2.3]\n",
      "Example 119: label 2, features [ 6.   2.2  5.   1.5]\n",
      "Example 120: label 2, features [ 6.9  3.2  5.7  2.3]\n",
      "Example 121: label 2, features [ 5.6  2.8  4.9  2. ]\n",
      "Example 122: label 2, features [ 7.7  2.8  6.7  2. ]\n",
      "Example 123: label 2, features [ 6.3  2.7  4.9  1.8]\n",
      "Example 124: label 2, features [ 6.7  3.3  5.7  2.1]\n",
      "Example 125: label 2, features [ 7.2  3.2  6.   1.8]\n",
      "Example 126: label 2, features [ 6.2  2.8  4.8  1.8]\n",
      "Example 127: label 2, features [ 6.1  3.   4.9  1.8]\n",
      "Example 128: label 2, features [ 6.4  2.8  5.6  2.1]\n",
      "Example 129: label 2, features [ 7.2  3.   5.8  1.6]\n",
      "Example 130: label 2, features [ 7.4  2.8  6.1  1.9]\n",
      "Example 131: label 2, features [ 7.9  3.8  6.4  2. ]\n",
      "Example 132: label 2, features [ 6.4  2.8  5.6  2.2]\n",
      "Example 133: label 2, features [ 6.3  2.8  5.1  1.5]\n",
      "Example 134: label 2, features [ 6.1  2.6  5.6  1.4]\n",
      "Example 135: label 2, features [ 7.7  3.   6.1  2.3]\n",
      "Example 136: label 2, features [ 6.3  3.4  5.6  2.4]\n",
      "Example 137: label 2, features [ 6.4  3.1  5.5  1.8]\n",
      "Example 138: label 2, features [ 6.   3.   4.8  1.8]\n",
      "Example 139: label 2, features [ 6.9  3.1  5.4  2.1]\n",
      "Example 140: label 2, features [ 6.7  3.1  5.6  2.4]\n",
      "Example 141: label 2, features [ 6.9  3.1  5.1  2.3]\n",
      "Example 142: label 2, features [ 5.8  2.7  5.1  1.9]\n",
      "Example 143: label 2, features [ 6.8  3.2  5.9  2.3]\n",
      "Example 144: label 2, features [ 6.7  3.3  5.7  2.5]\n",
      "Example 145: label 2, features [ 6.7  3.   5.2  2.3]\n",
      "Example 146: label 2, features [ 6.3  2.5  5.   1.9]\n",
      "Example 147: label 2, features [ 6.5  3.   5.2  2. ]\n",
      "Example 148: label 2, features [ 6.2  3.4  5.4  2.3]\n",
      "Example 149: label 2, features [ 5.9  3.   5.1  1.8]\n",
      "[0 1 2]\n",
      "[0 1 2]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Part 2 - Visualizing a Decision Tree - https://youtu.be/tNa99PG8hR8\n",
    "\n",
    "# Build one on a real dataset, add code to visualize it, and practice reading it - so you can see how it works under the\n",
    "# hood.\n",
    "\n",
    "# Use Iris flower data set: https://en.wikipedia.org/wiki/Iris_flower_data_set\n",
    "# Identify type of flower based on measurements\n",
    "# Dataset includes 3 species of Iris flowers: setosa, versicolor, virginica\n",
    "# 4 features used to describe: length and width of sepal and petal\n",
    "# 50 examples of each type for 150 total examples\n",
    "\n",
    "# Goals\n",
    "# 1-Import dataset\n",
    "# 2-Train a classifier\n",
    "# 3-Predict label for new flower\n",
    "# 4-Visualize the tree\n",
    "\n",
    "# scikit-learn datasets: http://scikit-learn.org/stable/datasets/\n",
    "# already includes Iris dataset: load_iris\n",
    "\n",
    "from sklearn.datasets import load_iris\n",
    "\n",
    "iris = load_iris()\n",
    "print dir(iris)\n",
    "print(type(iris))\n",
    "print iris.feature_names  # metadata: names of the features\n",
    "print iris.target_names  # metadata: names of the different types of flowers\n",
    "# print iris.data  # features and examples themselves\n",
    "print iris.data[0]  # first flower\n",
    "print iris.target[0]  # contains the labels\n",
    "\n",
    "#print entire dataset\n",
    "for i in xrange(len(iris.target)):\n",
    "    print \"Example %d: label %s, features %s\" % (i, iris.target[i], iris.data[i])\n",
    "\n",
    "# Testing Data\n",
    "# Examples used to test the classifier's accuracy\n",
    "# Not part of the training data\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn import tree\n",
    "\n",
    "iris = load_iris()\n",
    "# here, we remove the first example of each flower\n",
    "# found at indices: 0, 50, 100\n",
    "test_idx = [0, 50, 100]\n",
    "\n",
    "# create 2 new sets of variables, for training and testing\n",
    "# training data\n",
    "# remove the entires from the data and target variables\n",
    "train_target = np.delete(iris.target, test_idx)\n",
    "train_data = np.delete(iris.data, test_idx, axis=0)\n",
    "\n",
    "# testing data\n",
    "test_target = iris.target[test_idx]\n",
    "test_data = iris.data[test_idx]\n",
    "\n",
    "# create new classifier\n",
    "clf = tree.DecisionTreeClassifier()\n",
    "# train on training data\n",
    "clf.fit(train_data, train_target)\n",
    "\n",
    "# what we expect\n",
    "print test_target\n",
    "# what tree predicts\n",
    "print clf.predict(test_data)\n",
    "\n",
    "# Visualize\n",
    "# from scikit decision tree tutorial: http://scikit-learn.org/stable/modules/tree.html\n",
    "from sklearn.externals.six import StringIO\n",
    "import pydot\n",
    "\n",
    "dot_data = StringIO()\n",
    "tree.export_graphviz(clf, out_file=dot_data,\n",
    "                     feature_names=iris.feature_names,\n",
    "                     class_names=iris.target_names,\n",
    "                     filled=True, rounded=True,\n",
    "                     impurity=False)\n",
    "graph = pydot.graph_from_dot_data(dot_data.getvalue())\n",
    "graph.write_pdf(\"iris.pdf\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-04T01:10:28.218992Z",
     "start_time": "2018-01-04T01:10:27.475138Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAX0AAAD8CAYAAACb4nSYAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f85895e8390>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Part 3 - What Makes a Good Feature? - https://youtu.be/N9fDIAflCMY\n",
    "\n",
    "# Good features are informative, independent, and simple. We'll introduce these concepts by using a histogram to\n",
    "# visualize a feature from a toy dataset.\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Create population of 1000 dog, 50/50 greyhound/labrador\n",
    "greyhounds = 500\n",
    "labs = 500\n",
    "\n",
    "# Assume greyhounds are normally 28\" tall\n",
    "# Assume labradors are normally 24\" tall\n",
    "# Assume normal distribution of +/- 4\"\n",
    "grey_height = 28 + 4 * np.random.randn(greyhounds)\n",
    "lab_height = 24 + 4 * np.random.randn(labs)\n",
    "\n",
    "# Greyounds - red, labradors - blue\n",
    "plt.hist([grey_height, lab_height], stacked=True, color=['r', 'b'])\n",
    "plt.show()\n",
    "\n",
    "# Independent features are best\n",
    "# Avoid redundant features (height in in AND height in cm)\n",
    "\n",
    "# Ideal features are:\n",
    "# Informative\n",
    "# Independent\n",
    "# Simple\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-04T01:10:54.788618Z",
     "start_time": "2018-01-04T01:10:54.739685Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2 1 0 1 2 0 2 2 0 0 1 2 0 0 2 2 0 1 0 2 2 1 2 2 0 1 0 0 1 0 1 1 1 1 2 2 0\n",
      " 2 1 0 1 1 2 2 0 1 1 2 0 2 2 1 0 0 0 1 2 1 0 2 1 2 1 1 0 2 2 2 1 0 2 2 0 1\n",
      " 1]\n",
      "0.96\n",
      "[2 1 0 1 2 0 2 2 0 0 1 2 0 0 2 2 0 1 0 2 2 1 2 2 0 1 0 0 1 0 1 1 2 1 2 2 0\n",
      " 2 1 0 1 1 2 2 0 1 1 2 0 2 2 1 0 0 0 1 2 1 0 2 1 2 1 1 0 2 2 2 1 0 2 2 0 1\n",
      " 1]\n",
      "0.973333333333\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python2.7/dist-packages/sklearn/cross_validation.py:41: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "# Let's Write a Pipeline - Machine Learning Recipes #4 - https://youtu.be/84gqSbLcBFE\n",
    "\n",
    "# How to test a model and determine accuracy\n",
    "\n",
    "# Partition data into 2 sets, train and test\n",
    "\n",
    "# import a dataset\n",
    "from sklearn import datasets\n",
    "\n",
    "iris = datasets.load_iris()\n",
    "\n",
    "# Can think of classifier as a function f(x) = y\n",
    "X = iris.data  # features\n",
    "y = iris.target  # labels\n",
    "\n",
    "# partition into training and testing sets\n",
    "from sklearn.cross_validation import train_test_split\n",
    "\n",
    "# test_size=0.5 -> split in half\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.5)\n",
    "\n",
    "# Classifier\n",
    "from sklearn import tree\n",
    "\n",
    "my_classifier = tree.DecisionTreeClassifier()\n",
    "my_classifier.fit(X_train, y_train)\n",
    "\n",
    "# predict\n",
    "predictions = my_classifier.predict(X_test)\n",
    "print predictions\n",
    "\n",
    "# test\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "print accuracy_score(y_test, predictions)\n",
    "\n",
    "# Repeat using KNN\n",
    "# Classifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "my_classifier = KNeighborsClassifier()\n",
    "my_classifier.fit(X_train, y_train)\n",
    "\n",
    "# predict\n",
    "predictions = my_classifier.predict(X_test)\n",
    "print predictions\n",
    "\n",
    "# test\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "print accuracy_score(y_test, predictions)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-04T01:11:27.089154Z",
     "start_time": "2018-01-04T01:11:26.884946Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.92\n"
     ]
    }
   ],
   "source": [
    "# Writing Our First Classifier - Machine Learning Recipes #5 - https://youtu.be/AoeEHqVSNOw\n",
    "\n",
    "from scipy.spatial import distance\n",
    "\n",
    "\n",
    "def euc(a, b):\n",
    "    return distance.euclidean(a, b)\n",
    "\n",
    "\n",
    "class ScrappyKNN():\n",
    "    \"\"\"\n",
    "    Barebones KNN\n",
    "    \"\"\"\n",
    "\n",
    "    def fit(self, X_train, y_train):\n",
    "        \"\"\"\n",
    "        Takes features and labels for training set as input\n",
    "        :param X_train:\n",
    "        :param y_train:\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        self.X_train = X_train\n",
    "        self.y_train = y_train\n",
    "\n",
    "    def predict(self, X_test):\n",
    "        \"\"\"\n",
    "        Receives features for testing data\n",
    "        Output predictions for labels\n",
    "        :param X_test:\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        predictions = []\n",
    "        for row in X_test:\n",
    "            # label = random.choice(self.y_train)  # Random decision\n",
    "            label = self.closest(row)\n",
    "            predictions.append(label)\n",
    "\n",
    "        return predictions\n",
    "\n",
    "    def closest(self, row):\n",
    "        \"\"\"\n",
    "        Find the closest training point\n",
    "        :param row:\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        # Distance from test point to first training point\n",
    "        best_dist = euc(row, self.X_train[0])  # Shortest distance found so far\n",
    "        best_index = 0  # index of closest training point\n",
    "        for i in xrange(1, len(self.X_train)):  # Iterate over all other training points\n",
    "            dist = euc(row, self.X_train[i])\n",
    "            if dist < best_dist:  # Found closer, update\n",
    "                best_dist = dist\n",
    "                best_index = i\n",
    "        return self.y_train[best_index]  # closest example\n",
    "\n",
    "\n",
    "from sklearn import datasets\n",
    "\n",
    "iris = datasets.load_iris()\n",
    "\n",
    "X = iris.data\n",
    "y = iris.target\n",
    "\n",
    "from sklearn.cross_validation import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.5)\n",
    "\n",
    "# from sklearn.neighbors import KNeighborsClassifier\n",
    "my_classifier = ScrappyKNN()\n",
    "\n",
    "my_classifier.fit(X_train, y_train)\n",
    "\n",
    "predictions = my_classifier.predict(X_test)\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "print accuracy_score(y_test, predictions)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
